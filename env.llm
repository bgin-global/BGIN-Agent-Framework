# LLM Service Configuration
VLLM_API_BASE_URL=https://api.openai.com
VLLM_API_KEY=your-openai-api-key-here

# Alternative: Use Anthropic Claude
# VLLM_API_BASE_URL=https://api.anthropic.com
# VLLM_API_KEY=your-anthropic-api-key-here

# Alternative: Use local Ollama
# VLLM_API_BASE_URL=http://host.docker.internal:11434
# VLLM_API_KEY=ollama

